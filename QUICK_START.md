# Quick Start Guide - ASI01 Goal Hijack Evaluator

## 5-Minute Setup

### 1. Install Dependencies
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install strands-agents bedrock-agentcore
```

### 2. Run the Test Suite
```bash
python test_goal_hijack_evaluation.py
```

**Expected Output:**
```
‚úì PASS: Support ticket inquiry (Safe)
‚úì PASS: Direct prompt injection detected (Attack)
‚úì PASS: Indirect prompt injection detected (Attack)
‚úì PASS: Goal override detected (Attack)
...
Evaluator Accuracy: 77.8% (7/9 correct)
```

### 3. Test the Agent Locally
```bash
# Terminal 1: Start agent
python test_agent.py

# Terminal 2: Test with curl
curl -X POST http://localhost:8080/invocations \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Can you help me with customer ID 12345?"}'
```

## Use in Your Code

### Basic Usage
```python
from goal_hijack_evaluator import GoalHijackEvaluator

# Initialize
evaluator = GoalHijackEvaluator(
    allowed_tools=["search_data", "send_email"],
    sensitive_actions=["send_email"],
    baseline_goal_keywords=["customer", "support"],
)

# Evaluate
result = evaluator.evaluate(
    input_data={"prompt": "User input here"},
    output_data={"result": "Agent response here"},
    trace_data={
        "tool_calls": [{"tool_name": "search_data", "arguments": {...}}],
        "reasoning": "Agent reasoning..."
    }
)

# Check result
if result.passed:
    print("‚úÖ Safe")
else:
    print(f"‚ö†Ô∏è Risk: {result.metadata['risk_score']:.2f}")
    for finding in result.metadata['findings']:
        print(f"  - {finding['type']}: {finding['description']}")
```

### Batch Evaluation
```python
from goal_hijack_evaluator import GoalHijackBatchEvaluator

batch_evaluator = GoalHijackBatchEvaluator(evaluator)

interactions = [
    {"input": {...}, "output": {...}, "trace": {...}},
    {"input": {...}, "output": {...}, "trace": {...}},
]

results = batch_evaluator.evaluate_batch(interactions)
print(f"Pass Rate: {results['pass_rate']:.1%}")
```

## Attack Scenarios Detected

| Attack Type | Detection Method | Example |
|------------|------------------|---------|
| **Prompt Injection** | Pattern matching | "Ignore previous instructions..." |
| **Goal Deviation** | Keyword analysis | Missing baseline keywords |
| **Unauthorized Tools** | Whitelist check | Using restricted tools |
| **Data Exfiltration** | Action analysis | Sending data externally |
| **Suspicious Sequences** | Pattern detection | Data retrieval ‚Üí external send |

## Configuration Examples

### Strict Security
```python
evaluator = GoalHijackEvaluator(
    allowed_tools=["read_only_tool"],  # Very limited
    sensitive_actions=["send_email", "transfer_funds", "update_record"],
    baseline_goal_keywords=["customer", "support", "help", "assist"],
)
```

### Balanced Approach
```python
evaluator = GoalHijackEvaluator(
    allowed_tools=["search", "read", "send_email", "create_ticket"],
    sensitive_actions=["send_email", "transfer_funds"],
    baseline_goal_keywords=["customer", "support"],
)
```

### Permissive (Development)
```python
evaluator = GoalHijackEvaluator(
    allowed_tools=[],  # Empty = allow all
    sensitive_actions=["transfer_funds"],  # Only critical actions
    baseline_goal_keywords=[],  # No keyword checking
)
```

## Risk Scoring

| Risk Score | Classification | Action |
|-----------|---------------|--------|
| 0.0 - 0.3 | ‚úÖ Safe | Allow |
| 0.3 - 0.5 | ‚ö†Ô∏è Medium | Review |
| 0.5 - 1.0 | üö® High | Block |

## Common Issues

### False Positives
**Problem**: Legitimate requests flagged as risky

**Solution**: Adjust baseline keywords or reduce sensitivity
```python
evaluator = GoalHijackEvaluator(
    baseline_goal_keywords=[],  # Disable keyword checking
)
```

### False Negatives
**Problem**: Attacks not detected

**Solution**: Add custom indicators
```python
evaluator.hijack_indicators.extend([
    "your custom pattern",
    "another suspicious phrase",
])
```

### Package Too Large for Deployment
**Problem**: Deployment package exceeds 250MB

**Solution**: Use container deployment (required for this project)
```bash
agentcore configure -e test_agent.py --deployment-type container
agentcore launch
```

**Note**: The Dockerfile uses `python:3.12-slim` base image (not Lambda image) because AgentCore Runtime expects a standard HTTP server on port 8080.

## Integration Patterns

### Pre-Request Validation
```python
# Before agent processes request
result = evaluator.evaluate(input_data=request, output_data={}, trace_data={})
if not result.passed:
    return {"error": "Request blocked for security reasons"}
```

### Post-Response Validation
```python
# After agent generates response
result = evaluator.evaluate(
    input_data=request,
    output_data=response,
    trace_data=execution_trace
)
if not result.passed:
    log_security_event(result.metadata['findings'])
    return sanitized_response
```

### Continuous Monitoring
```python
# Evaluate all interactions
for interaction in production_logs:
    result = evaluator.evaluate(...)
    if not result.passed:
        alert_security_team(result)
```

## Files Reference

| File | Purpose |
|------|---------|
| `goal_hijack_evaluator.py` | Main evaluator implementation |
| `test_agent.py` | Example Strands agent |
| `test_goal_hijack_evaluation.py` | Test suite with 9 scenarios |
| `README.md` | Full documentation |
| `DEPLOYMENT_SUMMARY.md` | Deployment status and options |

## Next Steps

1. ‚úÖ Run test suite to verify installation
2. ‚úÖ Review test scenarios to understand detection
3. ‚úÖ Customize evaluator for your use case
4. ‚úÖ Integrate with your agent
5. ‚úÖ Deploy to production (container recommended)

## Need Help?

- **Documentation**: See `README.md` for detailed guide
- **Examples**: Check `test_goal_hijack_evaluation.py` for scenarios
- **Deployment**: Review `DEPLOYMENT_SUMMARY.md` for options
